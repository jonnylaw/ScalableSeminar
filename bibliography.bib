@article{Gordon1993,
abstract = {An algorithm, the bootstrap filter, is proposed for implementing$\backslash$nrecursive Bayesian filters. The required density of the state vector is$\backslash$nrepresented as a set of random samples, which are updated and propagated$\backslash$nby the algorithm. The method is not restricted by assumptions of$\backslash$nlinearity or Gaussian noise: it may be applied to any state transition$\backslash$nor measurement model. A simulation example of the bearings only tracking$\backslash$nproblem is presented. This simulation includes schemes for improving the$\backslash$nefficiency of the basic algorithm. For this example, the performance of$\backslash$nthe bootstrap filter is greatly superior to the standard extended Kalman$\backslash$nfilter},
author = {Gordon, N.J. and Salmond, D.J. and a.F.M. Smith},
doi = {10.1049/ip-f-2.1993.0015},
file = {:Users/jonny/Library/Application Support/Mendeley Desktop/Downloaded/Gordon, Salmond, Smith - 1993 - Novel approach to nonlinearnon-Gaussian Bayesian state estimation(2).pdf:pdf},
isbn = {0956-375X},
issn = {0956375X},
journal = {IEE Proceedings F Radar and Signal Processing},
keywords = {bayesian filter,kalman filter,sequential estimation},
mendeley-groups = {ComposableModels},
number = {2},
pages = {107},
pmid = {19254492},
title = {{Novel approach to nonlinear/non-Gaussian Bayesian state estimation}},
volume = {140},
year = {1993}
},
@article{Andrieu2009,
abstract = {We introduce a powerful and flexible MCMC algorithm for stochastic simulation. The method builds on a pseudo-marginal method originally introduced in [Genetics 164 (2003) 1139-1160], showing how algorithms which are approximations to an idealized marginal algorithm, can share the same marginal stationary distribution as the idealized method. Theoretical results are given describing the convergence properties of the proposed method, and simple numerical examples are given to illustrate the promising empirical characteristics of the technique. Interesting comparisons with a more obvious, but inexact, Monte Carlo approximation to the marginal algorithm, are also given.},
archivePrefix = {arXiv},
arxivId = {0903.5480},
author = {Andrieu, Christophe and Roberts, Gareth O.},
doi = {10.1214/07-AOS574},
eprint = {0903.5480},
isbn = {00905364 (ISSN)},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Auxiliary variable,Convergence,Marginal,Markov chain Monte Carlo},
mendeley-groups = {Bayesian,ComposableModels},
number = {2},
pages = {697--725},
title = {{The pseudo-marginal approach for efficient Monte Carlo computations}},
volume = {37},
year = {2009}
},
@article{liu2001combined,
  title={Combined parameter and state estimation in simulation-based filtering},
  author={Liu, Jane and West, Mike},
  booktitle={Sequential Monte Carlo methods in practice},
  pages={197--223},
  year={2001},
  publisher={Springer}
},
@article{Carvalho2010,
abstract = {Particle learning (PL) provides state filtering, sequential parameter learning and smoothing in a general class of state space models. Our approach extends existing particle methods by incorporating the estimation of static parameters via a fully-adapted filter that utilizes conditional sufficient statistics for parameters and/or states as particles. State smoothing in the presence of parameter uncertainty is also solved as a by-product of PL. In a number of examples, we show that PL outperforms existing particle filtering alternatives and proves to be a competitor to MCMC.},
archivePrefix = {arXiv},
arxivId = {1011.1098},
author = {Carvalho, Carlos M. and Johannes, Michael S. and Lopes, Hedibert F. and Polson, Nicholas G.},
doi = {10.1214/10-STS325},
eprint = {1011.1098},
file = {:Volumes/Jonny's Drive/Dropbox/Study/CCBDCDT/Journal Articles/ParticleLearning.pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Mixture Kalman filter,and phrases,learning,mixture kalman filter,parameter learning,particle,particle learning,sequential inference,smoothing,state filtering,state space models},
mendeley-groups = {ComposableModels},
number = {1},
pages = {88--106},
title = {{Particle Learning and Smoothing}},
url = {http://arxiv.org/abs/1011.1098},
volume = {25},
year = {2010}
},
@article{Storvik2002,
abstract = {Particle filters for dynamic state-space models handling unknown$\backslash$nstatic parameters are discussed. The approach is based on marginalizing$\backslash$nthe static parameters out of the posterior distribution such that only$\backslash$nthe state vector needs to be considered. Such a marginalization can$\backslash$nalways be applied. However, real-time applications are only possible$\backslash$nwhen the distribution of the unknown parameters given both observations$\backslash$nand the hidden state vector depends on some low-dimensional sufficient$\backslash$nstatistics. Such sufficient statistics are present in many of the$\backslash$ncommonly used state-space models. Marginalizing the static parameters$\backslash$navoids the problem of impoverishment, which typically occurs when static$\backslash$nparameters are included as part of the state vector. The filters are$\backslash$ntested on several different models, with promising results},
author = {Storvik, Geir},
doi = {10.1109/78.978383},
file = {:Users/jonny/Downloads/00978383.pdf:pdf},
isbn = {1053-587X},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Global parameters,Marginalization,Particle filters,Sequential updating,State-space models,Sufficient statistics},
mendeley-groups = {ComposableModels},
number = {2},
pages = {281--289},
title = {{Particle filters for state-space models with the presence of unknown static parameters}},
volume = {50},
year = {2002}
}



