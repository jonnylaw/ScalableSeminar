---
title: "Scalable Bayesian Inference for Streaming Data"
author: "Jonathan Law"
date: "2 November 2016"
autosize: true
bibliography: bibliography.bib
output: 
  ioslides_presentation:
    logo: Figures/NewcastleLogo.svg
    css: my_presentation.css
---
  
```{r setup, include=FALSE}
packages <- c("dplyr", "tidyr", "ggplot2", "gridExtra", "magrittr", "scales", "ggfortify", "leaflet", "readr")
newPackages <- packages[!(packages %in% as.character(installed.packages()[,"Package"]))]
if(length(newPackages)) install.packages(newPackages)
lapply(packages,require,character.only=T)

theme_set(theme_minimal())

## Connect to the urban observatory database
## my_db = src_postgres(dbname = "uodata_2", host = "uodata1", port = "5432", user = "uo_select")
```

## Streaming Data

* There are many sources of streaming data
    + Streaming video: Netflix, YouTube, Amazon Video etc
    + Social Media: Twitter, Facebook etc
    + Connected Devices, the internet of things
    
* Streaming data is large, typically hetergeneous and typically correlated in time

* Analysing this data online is desirable; reducing storage and computational costs and allowing decisions to be made more quickly

## Urban Observatory Air Quality Data

```{r, eval=FALSE, echo=FALSE}
# Get the sensor locations
query = paste0(readLines("Scripts/GetSensorLocations.SQL"), collapse = " ")
locations = tbl(my_db, sql(query)) %>% collect()
write.csv(locations, file = "locations.csv", row.names = FALSE)
```

```{r map, echo=FALSE, message=FALSE, fig.align='center'}
locations = read_csv("locations.csv")

## Output a leaflet map of sensor locations
## Remove misspecified sensor locations (with lat & lon = 0.0)
leaflet(locations %>% filter(latitude != 0.0)) %>%
  addTiles() %>%
  addMarkers(~longitude, ~latitude, popup = ~ paste(name, reading, sep = "\n"))
```

<div class="notes">
* Total Air Quality Sensors: 
* Total Readings from Air Quality sensor 03/11/2016: 316,142
* Total Readings 03/11/2016: 984708
</div>

<!-- ## Urban Observatory Readings -->

<!-- ```{r, eval=FALSE, echo=FALSE} -->
<!-- totalReadingsByTheme = tbl(my_db, sql("SELECT theme, COUNT(*) AS total_readings, date_trunc('month', timestamp) AS date FROM sensor_data_formatted GROUP BY theme, date_trunc('month', timestamp)")) -->

<!-- totalReadingsByTheme %>% -->
<!--   write.csv(file = "readings_by_month.csv", row.names = FALSE) -->
<!-- ``` -->

<!-- ```{r, echo=FALSE, message=FALSE, fig.align='center', eval=FALSE} -->
<!-- totalReadingsByTheme = read_csv("readings_by_day.csv") -->

<!-- totalReadingsByTheme %>% -->
<!--   filter(theme != "NA") %>% -->
<!--   ggplot(aes(x = date, y = total)) +  -->
<!--   geom_line() + facet_wrap(~theme) -->
<!-- ``` -->

## Urban Observatory Readings

```{r ts_plot, cache=TRUE, echo=FALSE, message=FALSE, fig.align="center"}
# Set parameters for data
readings = c("Temperature", "NO2", "CO", "NO", "Humidity")
name = "new_new_emote_1102"
startDate = Sys.Date() - 2
endDate = Sys.Date()

# tbl(my_db, sql("SELECT * FROM sensor_data_formatted")) %>%
# filter(sensor_name == name & timestamp >= startDate & timestamp <= endDate & reading %in% readings) %>%
# write.csv("sensor_data.csv", row.names = FALSE)

data = read_csv("sensor_data.csv")

# Plot the readings
data %>%
  collect() %>%
  mutate(numeric_value = as.numeric(value)) %>%
  ggplot(aes(x = timestamp, y = numeric_value, linetype = reading)) +
  geom_line() +
  ggtitle(paste0(paste0(readings, collapse = ", "), " readings at\n sensor ", name)) +
  ylab("reading") +
  theme(legend.position = "none") +
  facet_wrap(~reading, ncol = 1, scales = "free_y")
```

## Bayesian Parametric Inference

* Given data, $y$, we want to fit a model with parameters $\theta$
  
* Bayes theorem allows us to do this

$$p(\theta | y) = \frac{p(\theta)p(y | \theta)}{p(y)} $$
  
* $p(\theta | y)$ is the posterior distribution, $p(\theta)$ is the prior distribution, $p(y|\theta$) the likelihood and $p(y) = \int_\theta p(y | \theta) p(\theta) \textrm{d}\theta$
  
* Once parameters have been determined, the parametric model can be used to answer questions about the process which generated the data, $y$
  
## Streaming Data
  
* Streaming data is observed as a chronological sequence of values, $Y$, with an associated timestamp, $t_i$

$$ Y(t_{0:N}) = \{y(t_0), y(t_1), \dots , y(t_N) \} $$

* The Bayesian framework is natural for streaming time series data

* We want to forecast future observations: $y(t_k), k > N$

* We do this by fitting a parametric model which describes the process

## Regularly Observed Data

* Streaming data is often irregularly observed, consider first a completely observed discrete sensor for a lightbulb

* The lightbulb can be considered either on, or off, here is a regularly observed series of the lightbulb

```{r lightbulb, echo=FALSE, fig.width=5, fig.height=3, fig.align="center"}
set.seed(123453631)
lightbulb_data = data_frame(t = seq(as.POSIXct("2016-09-01"), as.POSIXct("2016-09-01 12:00"), length.out = 12*12), 
                            y = cumsum(rnorm(12*12, 0.5, 1.0)) < 2.0) 

lightbulb_data %>%
  ggplot(aes(x = t, y = y)) + 
  geom_step() + geom_point(size = 0.5) + theme_bw() +
  xlab("Time Of Day") +
  ylab("Lightbulb On") +
  scale_x_datetime(labels = date_format("%H:%M"))
```

* The lightbulb reports its status every five minutes, leading to $\frac{60}{5} \times 24 = 288$ observations each day

## Time to Event Data

* Recording the event time can reduce storage costs

* In the regularly observed data there are 288 observations a day, in time to event, there is one observation

* Storing the data this way, we can accurately answer questions such as: "at 12pm on Tuesday, how many lights were on in The Core?"

```{r, echo=FALSE, fig.width=5, fig.height=3, fig.align="center"}
irregular_light_data = data_frame(t = c(as.POSIXct("2016-09-01"), as.POSIXct("2016-09-01 02:48:35")), y = c(TRUE, FALSE))

lightbulb_data %>%
  ggplot(aes(x = t, y = y)) +
  geom_step() +
  geom_point(data = irregular_light_data, aes(x = t, y = y)) + 
  xlab("Time Of Day") +
  ylab("Lightbulb On") +
  scale_x_datetime(labels = date_format("%H:%M"))
```

## Irregularly Observed Data

* Sometimes a continuous process can be observed at discrete irregular intervals

* This is typically the case with sensor data, sometimes sensors can go offline or change their sampling rate adaptively

## Modelling Irregularly Observed Data | POMP Models

* Partially Observed Markov Process model

$$\begin{align*}
y(t_i)|\eta(t_i) &\sim \pi(y(t_i) | \eta(t_i)), \\
\eta(t_i)|\textbf{x}(t_i) &= g(F_{t_i}^T \textbf{x}(t_i)), \\
\textbf{X}(t_i) | \textbf{x}(t_{i-1}) &\sim p(\textbf{x}(t_i) | \textbf{x}(t_{i-1}))
\end{align*}$$
  
* The observation distribution $\pi(.)$ is flexible
* The function $g$ permits a deterministic non-linear transformation of the state space
* $F_t$ is a time dependent vector representing a linear transformation
* The state space, $\textbf{x}(t)$ is a continuous time Markov process

## POMP Model | Representation of a POMP model as a directed acyclic graph (DAG)

<div align="center">![POMP](Figures/single-model.svg)</div>
  
## The State Space | Diffusion Process Plot, Generalised Brownian Motion
  
```{r sde, echo=FALSE, message=FALSE, eval=FALSE}
shinyApp(
  ui = fluidPage(
    sidebarLayout(
      sidebarPanel(
        sliderInput("mu", "Mu, the drift", min = -5.0, max = 5.0, value = 0.5, step = 0.1),
        sliderInput("sigma", "Sigma, the diffusion", min = 0.0, max = 5.0, value = 1.0, step = 0.1)
      ),
      mainPanel(
        plotOutput("brownian")        
      )
    )
  ),
  
  ## Plot some Brownian motion
  server = function(input, output) {
    output$brownian = renderPlot({
      ## Can this be replaced with inline scala, lol no
      brownian_sims = function(mu, sigma) {
        x = numeric(100)
        x[1] = rnorm(1, 0, 1)
        for (i in 2:100) {
          x[i] = rnorm(1, mean = x[i-1] + mu, sd = sigma)
        }
        data_frame(Time = 1:100, Value = x) %>%
          mutate(upper = qnorm(p = 0.975, mean = Value, sd = sigma), 
                 lower = qnorm(p = 0.025, mean = Value, sd = sigma))
      }
      
      brownianSims = brownian_sims(input$mu, input$sigma)
      
      brownianSims %>%
        ggplot(aes(x = Time, y = Value)) + 
        geom_line() +
        geom_ribbon(aes(ymax = upper, ymin = lower), alpha = 0.5) +
        ggtitle("Generalised Brownian Motion Simulation")
    })
  }
)
```

$$\textrm{d}X(t) = \mu X(t) \textrm{d}t + \sigma \textrm{d}W(t)$$

```{r, echo=FALSE, fig.align='center', fig.width = 5, fig.height=3}
brownian_sims = function(mu, sigma) {
  x = numeric(100)
  x[1] = rnorm(1, 0, 1)
  for (i in 2:100) {
    x[i] = rnorm(1, mean = x[i-1] + mu, sd = sigma)
  }
  data_frame(Time = 1:100, Value = x) %>%
    mutate(upper = qnorm(p = 0.975, mean = Value, sd = sigma), 
           lower = qnorm(p = 0.025, mean = Value, sd = sigma))
}

brownianSims = bind_rows(cbind(drift = rep(0.0, 100), brownian_sims(0.0, 0.5)),
                         cbind(drift = rep(0.1, 100), brownian_sims(0.1, 0.5)),
                         cbind(drift = rep(-0.1, 100), brownian_sims(-0.1, 0.5)),
                         cbind(drift = rep(0.5, 100), brownian_sims(0.5, 0.5)))

brownianSims %>%
  ggplot(aes(x = Time, y = Value)) + 
  geom_line() +
  geom_ribbon(aes(ymax = upper, ymin = lower), alpha = 0.3) +
  ggtitle("Generalised Brownian Motion Simulation With Different Drift Values") + 
  facet_wrap(~drift, scales = "free_y")
```

## The State Space | Diffusion Process Plot, Ornstein-Uhlenbeck Process

```{r OU, echo=FALSE, eval=FALSE}
shinyApp(
  ui = fluidPage(
    sidebarLayout(
      sidebarPanel( sliderInput("theta", "theta, the mean", min = -5.0, max = 5.0, value = 3.0, step = 0.1),
                    sliderInput("alpha", "alpha, mean reverting parameter", min = 0.05, max = 1.0, value = 0.1, step = 0.05),
                    sliderInput("sigma", "sigma, diffusion coefficient", min = 0.0, max = 3.0, value = 0.5, step = 0.1)),
      mainPanel(
        plotOutput("ou")
      )
    )
  ),
  
  server = function(input, output) {
    ## Plot the OU process
    output$ou = renderPlot({
      ou_sims = function(theta, alpha, sigma) {
        x = numeric(100)
        x[1] = rnorm(1, 0, 1)
        for (i in 2:100) {
          mean = theta + (x[i-1] - theta) * exp(- alpha)
          variance = (sigma**2/2*alpha)*(1-exp(-2*alpha))
          x[i] = rnorm(1, mean, sqrt(variance))
        }
        
        data_frame(Time = 1:100, Value = x)  %>%
          mutate(upper = qnorm(0.975, mean = Value, sd = sigma), 
                 lower = qnorm(0.025, mean = Value, sd = sigma))
      }
      
      ou_sims(input$theta, input$alpha, input$sigma) %>%
        ggplot(aes(x = Time, y = Value)) + 
        geom_line() +
        geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) +
        ggtitle("Ornstein-Uhlenbeck Process Simulation")
    })
  }
)
```

$$\textrm{d}X(t) = \alpha (\theta - X(t)) \textrm{d}t + \sigma \textrm{d}W(t)$$

```{r OU sims, echo=FALSE, fig.align="center", fig.width=5, fig.height=3}
ou_sims = function(theta, alpha, sigma) {
  x = numeric(100)
  x[1] = rnorm(1, 0, 1)
  for (i in 2:100) {
    mean = theta + (x[i-1] - theta) * exp(- alpha)
    variance = (sigma**2/2*alpha)*(1-exp(-2*alpha))
    x[i] = rnorm(1, mean, sqrt(variance))
  }
  
  data_frame(Time = 1:100, Value = x)  %>%
    mutate(upper = qnorm(0.975, mean = Value, sd = sigma), 
           lower = qnorm(0.025, mean = Value, sd = sigma))
}

ouSims = bind_rows(cbind(alpha = rep(0.05, 100), ou_sims(3.0, 0.05, 0.1)),
                   cbind(alpha = rep(0.1, 100), ou_sims(3.0, 0.1, 0.1)),
                   cbind(alpha = rep(0.2, 100), ou_sims(3.0, 0.2, 0.1)),
                   cbind(alpha = rep(0.5, 100), ou_sims(3.0, 0.5, 0.1)))

ouSims %>%
  ggplot(aes(x = Time, y = Value)) + 
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) +
  ggtitle("Ornstein-Uhlenbeck Process Simulation with Varying Values of Alpha") + 
  facet_wrap(~alpha)
```

## Example POMP Model | A Poisson Model

$$\begin{align*}
N(t) &\sim \textrm{Poisson}(N(t) | \lambda(t)) \\
\lambda(t_i) &= \exp\{ x(t) \} \\
\textrm{d}X(t) &= \mu X(t)\textrm{d}t + \sigma \textrm{d}W(t)
\end{align*}$$
  
```{r poissonPlot, echo=FALSE, message=FALSE, fig.width=5, fig.height=3, fig.align="center"}
## system("cd ~/Desktop/ComposableModels && sbt \"run-main com.github.jonnylaw.examples.SimulatePoisson\"")
poisson = read_csv("PoissonSims.csv", col_names = c("Time", "Observation", "Eta", "Gamma", "State"))

type = c("Observation", "Eta", "State")

poisson %>%
  select(-Gamma) %>%
  gather(key = "key", value = "value", -Time) %>%
  mutate(key = factor(key, levels = type)) %>%
  arrange(key) %>%
  ggplot(aes(x = Time, y = value, linetype = key)) + 
  geom_line() +
  theme(legend.position="none") +
  facet_wrap(~key, ncol = 1, scales = "free_y") +
  ggtitle("Poisson simulation")
```

## Composing POMP models | Modelling Complex Processes

* Define an associative binary composition operator for models
* The Left-hand observation model and linking function, $g$ are chosen
* The Linear transformation vectors are concatenated: $F^{(3)}_t = \begin{bmatrix} F^{(1)}_t \\ F^{(2)}_t \end{bmatrix}$
* The latent state of each model advances according to its state evolution equation:

$$ \textbf{X}(t_i) | \textbf{x}(t_{i-1}) \sim \begin{pmatrix} 
p_1(\textbf{x}^{(1)}(t_i) | \textbf{x}^{(1)}(t_{i-1})) \\
p_2(\textbf{x}^{(2)}(t_i) | \textbf{x}^{(2)}(t_{i-1}))
\end{pmatrix} $$

* The class of unparameterised models and the composition operator forms a semi group

## Composed POMP Example | A Seasonal Time To Event Model

* Closely related to the Poisson Model, is the Log-Gaussian Cox-Process

* Consider an inhomogeneous Poisson process, with rate $\lambda(t)$, then the total number of events in $(0,T]$ is:

$$N(t) \sim \textrm{Poisson}\left (\int_0^T \lambda(t) \textrm{d}s \right )$$

## Composed POMP Example | A Seasonal Time To Event Model

* Denote $\Lambda(t) = \int_0^t \lambda(s) \textrm{d}s$ and calculate the probability of observing at least one event:

$$\begin{align*}
P(N(t) > 0) &= 1 - p(N(t) = 0) \\
&= 1 - \frac{\Lambda(t)^0 e^{-\Lambda(t)}}{0!} \\
& = 1 - \exp \left \{ -\Lambda(t) \right \}
\end{align*}$$

* This is equivalent to the CDF of the LGCP, the PDF is given by:

$$f(t) = \frac{\textrm{d}}{\textrm{d}t}F(t) = \lambda(t) \exp \left \{-\Lambda(t) \right \}$$

## Composed POMP Example | A Seasonal Time to Event Model

* The LGCP model, with a generalised brownian motion state space is then:

$$\begin{align*}
p(t|\lambda(t)) &= \lambda(t) \exp \left \{-\Lambda(t) \right \} \\
\begin{pmatrix}
\lambda(t) \\
\textrm{d}\Lambda(t)
\end{pmatrix} &=\begin{pmatrix}
\exp \{x(t)\} \\
\lambda(t)\textrm{d}t
\end{pmatrix}\\
\textrm{d}X(t) &= \mu X(t)) \textrm{d}t + \sigma \textrm{d}W(t)
\end{align*}$$

* We can model irregularly observed events, such as the lightbulb series

* Using past data, we can estimate the parameters of a suitable model for the lighbulb process and predict the next time the light will turn on

## Composed POMP Example | A Seasonal Time To Event Model

* Define a seasonal model, with an Ornstein-Uhlenbeck state space

$$\begin{align*}
y(t) &\sim \mathcal{N}(y(t) | \mu(t), \sigma) \\
\mu(t) &= F_t^T \textbf{x}(t) \\
\textrm{d}\textbf{X}(t) &= \alpha(\theta - \textbf{X}(t)) \textrm{d}t + \Sigma \textrm{d}W(t)
\end{align*}$$

* $F_t^T$ is a time dependent vector of fourier components representing seasonality:

$$F_t = \begin{pmatrix}
\cos(\omega t) \\
\sin(\omega t) \\
\cos(2\omega t) \\
\sin(2\omega t) \\
\cos(3\omega t) \\
\sin(3\omega t)
\end{pmatrix}$$
  
## Composed Pomp Example | A Seasonal Time To Event Model
  
$$\begin{align*}
t &\sim \pi(t | \lambda(t), \Lambda(t)) \\
\begin{pmatrix}\lambda(t_i) \\
\Lambda(t_i)
\end{pmatrix} &= \begin{pmatrix} \exp\{ F_t^T x(t) \} \\
\lambda(t_i) \textrm{d}t
\end{pmatrix}\\
\textbf{X}(t_i)|\textbf{x}(t_{i-1}) &\sim \begin{pmatrix}
p_1(\textbf{x}^{(1)}(t_i) | \textbf{x}^{(1)}(t_{i-1})) \\
p_2(\textbf{x}^{(2)}(t_i) | \textbf{x}^{(2)}(t_{i-1}))
\end{pmatrix}
\end{align*}$$
  
```{r, echo=FALSE, message=FALSE, fig.align="center", fig.width=5, fig.height=3}
lgcp_seasonal = read_csv("seasonal_timetoevent.csv", col_names = c("Time", "Observation", "Eta", "Gamma", sapply(1:7, function(i) paste0("State", i))))

p1 = lgcp_seasonal %>%
  select(Time, Eta) %>%
  ggplot(aes(x = Time, y = Eta)) +
  geom_line()

p2 = lgcp_seasonal %>%
  select(Time, Observation) %>%
  filter(Observation == 1) %>%
  ggplot(aes(x = Time, y = Observation)) + 
  theme(axis.text.y=element_blank()) +
  geom_point()

grid.arrange(p2, p1, ncol = 1)
```

# Programming with Streams

## Stream | An infinite list

* `Stream`s can be thought of as an infinite list
* A Stream is pair, with the left hand value being the first element of the stream and the right hand value a thunk (a function with no arguments)
* Define a (infinite) stream of natural numbers using `from`

```scala
scala> val naturalNumbers = Stream.from(1)
```
`Stream(1, ?)`

## Akka Streams

* Akka Streams: A Scala library for stream processing

* Akka streams have three main abstractions:
  
* A `Source` is a definition of a stream, it can be a "pure" stream, or a database or webservice call

* A `Flow` is a processing stage, which can be used to transform a `Source` to another `Source`

* A `Sink` defines what happens at the end of the stream, usually an effect such as writing to a file

## Higher Order Functions | Operations on Streams

* `fold` can be used (with a seed) to reduce a `Stream` by recursively applying a binary operation

```scala
scala> Stream.from(1).
take(10).
fold(0)(_ + _)
```
`55`

* `fold` will reduce a stream from the left, `((1 + 2) + 3) + 4`

* `reduce` is equivalent to `fold` for associative operations and does not guarantee order of operations

## Higher Order Functions | Operations on Streams

* `scan` can be used to accumulate a running sum:
  
```scala
scala> Stream.from(1).
take(10).
scan(0)(_ + _)
```
`0, 1, 3, 6, 10, 15, 21, 28, 36, 45, 55`

## POMP models as a Stream

* The latent state is a Markov process, which means future observations only depend on the current observation

$$p(x(t_n) | x(t_{n-1}), \dots x(t_0)) = p(x(t_n) | x(t_{n-1}))$$
  
* In order to simulate the Markov Process, we can use the dual of `fold`, `unfold`, to simulate a random walk:

$$x(t) = x(t-1) + w(t), \quad w(t) \sim \mathcal{N}(0, W)$$

```scala
val x0 = Gaussian(0.0, 1.0).draw

Source.unfold(x0)(a => Some((Gaussian(a, sigma).draw, a)))
```

## Bayesian Inference with Streams | The Bootstrap Particle Filter

* In general, POMP models require simulation based filtering (such as the bootstrap filter [@Gordon1993]) to determine the behaviour of the latent state

* Let's describe a bootstrap filter for this Poisson model:

$$\begin{align*}
y(t_i) &\sim \textrm{Poisson}(y(t_i) | x(t_i)) \\
\lambda(t_i) &= \exp \{ x(t_i) \} \\
dX(t_i) &= \mu X(t) \textrm{d}t + \sigma \textrm{d}W(t)
\end{align*}$$

* Initialize: Sample $N$ particles from the state prior distribution, $x^{(i)}(t_0) \sim p(x(t_0))$, set the weights to $1/N$, $w^{(i)}(t_0) = 1/N$:

```scala
val n = 1000 // number of particles
val (x, w) = (Gaussian(0.0, 1.0).sample(n), Seq.fill(n)(1/n))
```

## Bayesian Inference with Streams | The Bootstrap Particle Filter

* Advance the particles to the time of the next observation, using the state transition model $x^{(i)}(t_k) \sim p(x^{(i)}(t_k) | x^{(i)}(t_{k-1}))$

```scala
def stepState(mu: Double, sigma: Double)
  (x: State, dt: TimeIncrement): State = {
    Gaussian(x + mu * dt, sigma * sigma * dt).draw
}
val x1 = x map (stepState(_, dt))
```

* Calculate the likelihood (weight) of each particle, given the observation at time $t_k$, $w^{(i)}(t_k) = \textrm{Poisson}(y(t_k) | \lambda(t_k)^{(i)}(t_k))$

```scala
val weights = x1 map (x => Poisson(exp(x)).pdf(observation))
```

## Bayesian Inference with Streams | The Bootstrap Particle Filter

* Resample the particles according to the weights, ie. select particle $j$ with probability $w^{(j)}(t_k)$

```scala
def resample(w: Seq[LogLikelihood], x: Seq[State]) = {
  Multinomial(w).sample(x.size) map (i => x(i))
}
```

* Store the approximate sample from the filtering distribution, $\hat{p}(x(t_k) | y(t_k))$

* Advance to the time of the next observation, and repeat without the intialization step

## Bayesian Inference with Streams | The Bootstrap Particle Filter

* Here's the full implementation of a single step of the bootstrap particle filter for the simple Poisson model

```scala
case class Data(time: Datetime, observation: Double)
case class FilterState(t0: Datetime, state: Seq[State])

def stepFilter(s: FilterState, d: Data): FilterState = {
  val dt = d.time - s.t0
  val state = s.state map (x => stepState(x, dt))
  val w = state map (x => Poisson(exp(x), v).logPdf(d.observation))

  FilterState(d.time, resample(w, state))
}
```

## Bayesian Inference With Streams | The Bootstrap Particle Filter
  
* In order to perform the bootstrap particle filter, we need to know the time of the previous observation and the previous particle cloud

* The the filter can by ran using an akka `Flow` and the `scan` operation, the initial state `init` contains the particle cloud and the time at the start of the application of the filter

```scala
def filter(init: FilterState) = 
  Flow[Data].scan(init)(stepFilter(v))

dataStream.
  via(filter(init)).
  runWith(Sink.foreach(println))
```

* The particle filter will be applied to the data stream one element at a time, a `Source` and print the result to the console as each datapoint is processed

## Bayesian Inference With Streams | Parallel Particle Filters

* In order to account for the uncertainty of the parameter estimates, we can sample from the parameter posterior $p(\theta | y(t_{1:N}))$ and run multiple particle filters

* Running multiple particle filters (in parallel) is trivial, assume `variance` is the posterior distribution of the variance:

```scala
Source(variance.sample(4).toList).
### <b>
  mapAsync(2){ v =>
### </b>
    Source(sims).
      take(100).
      via(filter(v, init)).
      runWith(Sink.ignore)
  }
```

* The full implementation of this can be found in the associated github repo [git.io/scalable](https://git.io/scalable)

## Illustration of Bootstrap Particle Filter

* The solid line represents the simulated state, the points represent the particle cloud with the opacity representing each particles likelihood

<div align="center">![AnimatedPf](Figures/animatedParticleFilter.gif)</div>

## Parameter Inference for POMP models

* **Offline Parameter Inference**:
    + MCMC algorithms: Particle Marginal Metropolis Hastings [@Andrieu2009]

* **Online Parameter Inference**:
    + Liu and West [@liu2001combined]
    + Storvik Filter [@Storvik2002]
    + Particle Learning [@Carvalho2010]
  
## Conclusion

* Bayesian Inference for irregularly observed time series
    + Improves accuracy of inference
    + Reduces computational cost
    + Regularly observed data is simply a special case

**Further Reading**

* I have written a Scala library which implements composable POMP models and inference methods
* Look at the code: [git.io/statespace](https://git.io/statespace)
* Read the paper: [arXiv:1609.00635](https://arxiv.org/abs/1609.00635)

## References
